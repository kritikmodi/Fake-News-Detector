{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getWikipediaFactsfin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheatcodesInc/Fake-news-detector/blob/main/getWikipediaFacts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install wikipedia\n",
        "# !wget -c \"https://figshare.com/ndownloader/files/13774556\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvEAPGMgeiY4",
        "outputId": "9212a078-3791-43fa-b7bc-919c5dbc912a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.8.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.21.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=b03007c484a54f4ca1e68cb2ab048cbc6574d8ec08c3dc41e069d26737c2ce06\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFic74_tnz8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c02362-5748-43d2-ea91-a4246c764954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "from pprint import pprint\n",
        "from nltk.util import ngrams\n",
        "import wikipedia as wiki\n",
        "import gensim\n",
        "import re\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "nlp = en_core_web_sm.load()\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use n-grams for search"
      ],
      "metadata": {
        "id": "D511dvtFtzMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def keywords(sentence):\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    results = ([(X.text, X.label_)[0] for X in doc.ents])\n",
        "    n_gram=[]\n",
        "\n",
        "    for i in range(2, 5):\n",
        "        n_gram.extend(ngrams(sentence.split(), i))\n",
        "\n",
        "    results.extend([' '.join(i) for i in n_gram])\n",
        "    return list(set(results))"
      ],
      "metadata": {
        "id": "I_U_tlGmoHQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_wiki_text(text):\n",
        "    text = re.sub(r'==.+==', '.', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'\\t', ' ', text)\n",
        "    text = re.sub(r'\\[[0-9]+\\]', ' ', text)\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "    text = re.sub(r'\\. \\.', '.', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "6m_tzHIdoQcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def content(claim, results):\n",
        "    sentences = []\n",
        "    found=[]\n",
        "    for i in results:\n",
        "        try :\n",
        "            current_page = wiki.page(i)\n",
        "            if current_page not in found:\n",
        "                found.append(current_page)\n",
        "        except :\n",
        "            continue\n",
        "    titles=[i.title for i in found]\n",
        "    titles=[i[0] for i in topNSimilar(claim, titles)]\n",
        "    for i in found:\n",
        "        if i.title not in titles:\n",
        "            found.remove(i)\n",
        "\n",
        "    for i in found:\n",
        "        current_content = i.content\n",
        "        sentences.extend(tokenize.sent_tokenize(clean_wiki_text(current_content)))\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "uql0zug-2_HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topNSimilar(claim, sentList, n=5):\n",
        "    distList=[]\n",
        "    for i in sentList:\n",
        "        document_term_matrix = TfidfVectorizer().fit_transform([i, claim])\n",
        "        dist = document_term_matrix * document_term_matrix.transpose()\n",
        "        distance = dist.toarray()[0][1]\n",
        "\n",
        "        if len(distList)<=n:\n",
        "            distList.append([i, distance])\n",
        "        else:\n",
        "            distList=sorted(distList, key=lambda x: x[1], reverse=True)\n",
        "            if distance>distList[-1][1]:\n",
        "                distList.pop()\n",
        "                distList.append([i, distance])\n",
        "    return sorted(distList, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "K7fGgbSXfRFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim = 'Donald Trump is the president of the united states'"
      ],
      "metadata": {
        "id": "OrcRCIW4fQrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = topNSimilar(claim, content(claim, keywords(claim)), 5)"
      ],
      "metadata": {
        "id": "o_izINgHfQMu",
        "outputId": "12da6ef9-ac79-4d14-f59a-a6e4d36445f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def truthfulness(claim, response):\n",
        "    "
      ],
      "metadata": {
        "id": "DPGM3o-lfPtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fKjikQ7ffPqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# \n"
      ],
      "metadata": {
        "id": "sv7Uu8ahlqOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=content(claim, keywords(claim))"
      ],
      "metadata": {
        "id": "Bc5L9NlunM4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res[1]"
      ],
      "metadata": {
        "id": "FTFjydMbtPRw",
        "outputId": "521f1361-4b24-40f7-f2fd-573f9d4bcc3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Vande Mataram',\n",
              " 'Written By',\n",
              " 'Bankim Chandra Chatterjee',\n",
              " 'It Was Written',\n",
              " 'Sarat Chandra Chattopadhyay']"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "for i in response:\n",
        "    sim_arr=cosine_similarity( [model.encode(claim)], [model.encode(i[0])] )[0][0]\n",
        "    print(i[0], sim_arr)"
      ],
      "metadata": {
        "id": "snCuZ5-DXzLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a67cdca-b169-4585-85b4-e493fb971f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The president of the United States (POTUS) is the head of state and head of government of the United States of America. 0.76244354\n",
            "The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces. 0.7018652\n",
            "The president has the power to nominate federal judges, including members of the United States courts of appeals and the Supreme Court of the United States. 0.62361306\n",
            "This is the only component in the inauguration ceremony mandated by the Constitution: I do solemnly swear (or affirm) that I will faithfully execute the Office of President of the United States, and will to the best of my ability, preserve, protect, and defend the Constitution of the United States. 0.5327108\n",
            "The position of the United States as the leading member of NATO, and the country's strong relationships with other wealthy or democratic nations like those comprising the European Union, have led to the moniker that the president is the \"leader of the free world.\" . 0.59828\n",
            "69:The President is to be commander-in-chief of the army and navy of the United States... 0.6445675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D4qr6oBkhRn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_9FpX0ObhRlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topNSimilar(claim, sentList, n=5):\n",
        "    distList=[]\n",
        "    for i in sentList:\n",
        "        document_term_matrix = TfidfVectorizer().fit_transform([i, claim])\n",
        "        dist = document_term_matrix * document_term_matrix.transpose()\n",
        "        distance = dist.toarray()[0][1]\n",
        "\n",
        "        if len(distList)<=n:\n",
        "            distList.append([i, distance])\n",
        "        else:\n",
        "            distList=sorted(distList, key=lambda x: x[1], reverse=True)\n",
        "            if distance>distList[-1][1]:\n",
        "                distList.pop()\n",
        "                distList.append([i, distance])\n",
        "    return sorted(distList, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "g9hkGgSxY2TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim = 'Vande Mataram was written by Bankim Chandra Chattopadhyay'"
      ],
      "metadata": {
        "id": "oCV4Av52YMXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topNSimilar(claim, content(claim, keywords(claim)), 5)"
      ],
      "metadata": {
        "id": "5yNLHzVAYMVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35cc09b1-899b-4199-9d04-08bc61278267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['If an anthem is defined as consisting of both a melody and lyrics, then the oldest national anthem in use today is the national anthem of the Netherlands, the Wilhelmus.',\n",
              "  0.47482540034883847],\n",
              " ['One of these is the \"Marcha Real\", the national anthem of Spain.',\n",
              "  0.46348639791287216],\n",
              " ['9 is the official anthem of the European Union and of the Council of Europe.',\n",
              "  0.41088847165649467],\n",
              " ['\"Esta É a Nossa Pátria Bem Amada\" is the national anthem of Guinea-Bissau and was also the national anthem of Cape Verde until 1996.',\n",
              "  0.4074540453346758],\n",
              " ['The national chair is the leading volunteer of the National Executive Board of the Boy Scouts of America, a position comparable to the chairman of a board of directors.',\n",
              "  0.4049238481992085],\n",
              " ['The national anthem of Pakistan, the \"Qaumi Taranah\", is unique in that it is entirely in Farsi (Persian) with the exception of one word which is in Urdu, the national language.',\n",
              "  0.3975677672553976]]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=topNSimilar(claim, res[0])"
      ],
      "metadata": {
        "id": "ulsodWVeeNrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles= [i.title for i in res[1]]"
      ],
      "metadata": {
        "id": "wFc9Yh0RzOKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles"
      ],
      "metadata": {
        "id": "0K7jXtbWzUQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f844aa-9f60-47b5-c40e-aaac3293e1b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Death of Subhas Chandra Bose',\n",
              " 'Subhas Chandra Bose',\n",
              " 'Aviation accidents and incidents',\n",
              " 'Ritchie Valens',\n",
              " 'Dior',\n",
              " 'Bachelor of Arts',\n",
              " 'Lynyrd Skynyrd plane crash']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4vn6V8sG64Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "data = res[0]\n",
        "\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]\n",
        "max_epochs = 80\n",
        "vec_size = 30\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=2,\n",
        "                dm =1,\n",
        "                epochs= max_epochs)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "oBgN98tUzcfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ba23cb5-4592-49c4-8f19-02f7d66e4744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
            "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=max_epochs)\n",
        "\n",
        "test_data = word_tokenize(claim.lower())\n",
        "v1 = model.infer_vector(test_data)\n",
        "\n",
        "# to find most similar doc using tags\n",
        "similar_doc = model.docvecs.most_similar('1')\n",
        "# print(similar_doc)\n",
        "\n",
        "\n",
        "# to find vector of doc in training data using tags or in other words, printing the vector of document at index 1 in training data\n",
        "# print(model.docvecs['1'])\n"
      ],
      "metadata": {
        "id": "RJyRH-Yh0-W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(data[int(similar_doc[i][0])])"
      ],
      "metadata": {
        "id": "f1A8CCptAMyc",
        "outputId": "f16fe306-8011-42d6-833b-b9fb6b7d09e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chattopadhyay was very intereste in recent events in Indian and Bengali history, particularly the Revolt of 1857 and the previous century's Sanyasi Rebellion.\n",
            "The poem was published in Chattopadhyay's book Anandamath (pronounced Anondomôţh in Bengali) in 1882, which is set in the events of the Sannyasi Rebellion.\n",
            "from Bengali by Surendranath Tagore.\n",
            "Durgeshnandini, his first Bengali romance and the first ever novel in Bengali, was published in 1865.\n",
            "His first attempt was a novel in Bengali submitted for a declared prize.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[print(i[0]) for i in result]"
      ],
      "metadata": {
        "id": "e-S-OlFYAYO9",
        "outputId": "35b6fec9-e4c8-461d-89a4-fe36284a47c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is generally believed that the concept of Vande Mataram came to Bankim Chandra Chattopadhyay when he was still a government official, around 1876.\n",
            "Media related to Bankim Chandra Chattopadhyay at Wikimedia Commons Quotations related to Bankim Chandra Chatterjee at Wikiquote Works by Bankim Chandra Chatterji at Project Gutenberg Works by or about Bankim Chandra Chatterjee at Internet Archive Works by Bankim Chandra Chatterjee at LibriVox (public domain audiobooks) https://en.banglapedia.org/index.php/Chattopadhyay,_Bankimchandra\n",
            "It Was Written was generally well received by critics.\n",
            "The first translation of Bankim Chandra Chattopadhyay's novel Anandamath, including the poem Vande Mataram, into English was by Nares Chandra Sen-Gupta, with the fifth edition published in 1906 titled \"The Abbey of Bliss\".Here is the translation in prose of the above two stanzas rendered by Sri Aurobindo Ghosh.\n",
            "Written By at IMDb\n",
            "Matangini Hazra's last words as she was shot to death by the Crown police were Vande Mataram.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WZj5saISKKn3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}