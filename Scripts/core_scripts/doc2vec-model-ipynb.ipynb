{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gensim\nimport nltk\nimport pandas as pd\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))\nlemma = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T22:42:51.716937Z","iopub.execute_input":"2022-08-23T22:42:51.717391Z","iopub.status.idle":"2022-08-23T22:42:54.643900Z","shell.execute_reply.started":"2022-08-23T22:42:51.717300Z","shell.execute_reply":"2022-08-23T22:42:54.642848Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def prep(rowitem):\n    if len(str(rowitem).split()) < 10:\n        return np.nan\n    rowitem = word_tokenize(str(rowitem))\n    rowitem = [i.lower() for i in rowitem if i.isalpha()]\n    rowitem = [ i for i in rowitem if i not in stop_words ]\n    rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n\n    return rowitem","metadata":{"execution":{"iopub.status.busy":"2022-08-23T22:42:54.645672Z","iopub.execute_input":"2022-08-23T22:42:54.646004Z","iopub.status.idle":"2022-08-23T22:42:54.653730Z","shell.execute_reply.started":"2022-08-23T22:42:54.645974Z","shell.execute_reply":"2022-08-23T22:42:54.652564Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/to-emb-or-not-to-emb/medium_test_data.csv').drop(columns=['Unnamed: 0', 'label'])\ndf.apply(prep)\ndf = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-08-23T22:42:54.655321Z","iopub.execute_input":"2022-08-23T22:42:54.655870Z","iopub.status.idle":"2022-08-23T22:42:56.790047Z","shell.execute_reply.started":"2022-08-23T22:42:54.655838Z","shell.execute_reply":"2022-08-23T22:42:56.789019Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data = df.content.values.tolist()\ntagged_data = [gensim.models.doc2vec.TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T22:42:56.792086Z","iopub.execute_input":"2022-08-23T22:42:56.793020Z","iopub.status.idle":"2022-08-23T22:43:04.587101Z","shell.execute_reply.started":"2022-08-23T22:42:56.792986Z","shell.execute_reply":"2022-08-23T22:43:04.585651Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"max_epochs = 5\nvec_size = 20\nalpha = 0.025\n\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size,\n                alpha=alpha, \n                min_alpha=0.00025,\n                min_count=1,\n                dm =1)\n  \nmodel.build_vocab(tagged_data) # Use data_for_training instead\n\nfor epoch in range(max_epochs):\n    print('iteration {0}'.format(epoch))\n    model.train(tagged_data, #  Use data_for_training instead\n                total_examples=model.corpus_count,\n                epochs=model.epochs)\n    # decrease the learning rate\n    model.alpha -= 0.0002\n    # fix the learning rate, no decay\n    model.min_alpha = model.alpha\n\nmodel.save(\"d2v.model\")\nprint(\"Model Saved\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T22:48:17.348512Z","iopub.execute_input":"2022-08-23T22:48:17.348927Z","iopub.status.idle":"2022-08-23T22:53:10.336115Z","shell.execute_reply.started":"2022-08-23T22:48:17.348895Z","shell.execute_reply":"2022-08-23T22:53:10.334667Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model= gensim.models.doc2vec.Doc2Vec.load(\"d2v.model\")\n\ntest_data = word_tokenize(\"the 8th edition of international day of yoga will see many firsts but one of\".lower())\nv1 = model.infer_vector(test_data)\n# print(\"V1_infer\", v1)\n\nsimilar_doc = model.docvecs.most_similar(positive=[v1])\nprint(similar_doc)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T22:54:26.643711Z","iopub.execute_input":"2022-08-23T22:54:26.644111Z","iopub.status.idle":"2022-08-23T22:54:27.224081Z","shell.execute_reply.started":"2022-08-23T22:54:26.644079Z","shell.execute_reply":"2022-08-23T22:54:27.221837Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"while True:\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-08-23T22:48:04.064872Z","iopub.status.idle":"2022-08-23T22:48:04.066191Z","shell.execute_reply.started":"2022-08-23T22:48:04.065966Z","shell.execute_reply":"2022-08-23T22:48:04.065990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}