{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-telegram-bot --upgrade\n!pip install tensorflow-text\n!pip install transformers\n!pip install fasttext\n!pip install scipy\n!pip3 install \"scikit_learn==0.22.2.post1\"\n!pip install tweepy==4.5.0\n!pip install sentence-transformers","metadata":{"id":"d_1OdULI4jPH","scrolled":true,"execution":{"iopub.status.busy":"2022-08-22T22:13:02.596256Z","iopub.execute_input":"2022-08-22T22:13:02.596650Z","iopub.status.idle":"2022-08-22T22:15:31.782422Z","shell.execute_reply.started":"2022-08-22T22:13:02.596571Z","shell.execute_reply":"2022-08-22T22:15:31.781121Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import requests\nimport base64\nimport json\nimport logging\nimport pandas as pd\nfrom telegram.ext import Updater , CommandHandler , MessageHandler , Filters\nfrom fastai.vision.all import load_learner\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom transformers import pipeline , AutoTokenizer, TFAutoModelForSequenceClassification\nimport fasttext\nimport numpy as np\nimport pickle\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nfrom pprint import pprint\nfrom nltk.util import ngrams\nimport gensim\nimport re\nimport nltk\nimport torch\nfrom nltk import tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sentence_transformers import SentenceTransformer\nimport tweepy as tweepy\n\n# nltk.download('stopwords')\n# nltk.download('omw-1.4')\n# nltk.download('punkt')\n# nltk.download('wordnet')","metadata":{"id":"kQ-_gg6f26ix","execution":{"iopub.status.busy":"2022-08-22T22:40:15.302161Z","iopub.execute_input":"2022-08-22T22:40:15.302791Z","iopub.status.idle":"2022-08-22T22:40:20.793087Z","shell.execute_reply.started":"2022-08-22T22:40:15.302757Z","shell.execute_reply":"2022-08-22T22:40:20.792092Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"APIKey = \"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\"\n!wget -O ./lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\nCONSUMER_KEY = \"eJL1xOgPnXVx0DzCr5pGa8lNv\"\nCONSUMER_SECRET = \"iDBuPdCEXZQDzsRqvNtkVcIhcdvlT8x8aW74VTm1EqXcIaPmrZ\"\nOAUTH_TOKEN = \"1420293020080082948-Qo8PBaf5oXA1xrPryabo3C3g09xdBf\"\nOAUTH_TOKEN_SECRET = \"HzW0KllX3NRls7pOKA0cPIbNEyAFWON9wgVODcRwlrVBi\"\nBEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAABKRewEAAAAAem8kRGNx2U5tGTmD%2BtikqmENETI%3DCqzEWLXGEmM8WKNXRRnW0Tke4QlWw2sihgwtjpVYwnAR0QD6bo\"\ntwitterAPI = tweepy.Client(bearer_token = BEARER_TOKEN)","metadata":{"id":"ultiAIXB50mk","execution":{"iopub.status.busy":"2022-08-22T22:40:20.794624Z","iopub.execute_input":"2022-08-22T22:40:20.795238Z","iopub.status.idle":"2022-08-22T22:40:28.000689Z","shell.execute_reply.started":"2022-08-22T22:40:20.795212Z","shell.execute_reply":"2022-08-22T22:40:27.999776Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def start(update, context) :\n    update.message.reply_text(\n        \"EN : Just give me a news and I will tell you whether it is FAKE or not\"\n    )\n\ndef help_command(update, context) :\n    update.message.reply_text('My only purpose is to tell you whether a given news is fake or not')\n\ndef load_models() :\n    global clickBaitModel , sentimentModel , biasModel , classification_after_embedding_model, cae_model_tokenizer, liar_classification_model\n    classification_after_embedding_model = TFAutoModelForSequenceClassification.from_pretrained('pururaj/Test_model')\n    cae_model_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n    clickBaitModel = pipeline(model=\"elozano/bert-base-cased-clickbait-news\", tokenizer=\"elozano/bert-base-cased-clickbait-news\")\n    sentimentModel = pipeline(model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n    biasModel = pipeline(model=\"d4data/bias-detection-model\", tokenizer=\"d4data/bias-detection-model\")\n    liar_classification_model = pickle.load(open('../input/inputdata/liar_classification.sav', 'rb'))\n\n# def load_model_second() :\n#     global wikipediaModel , stop_words , lemma\n#     model_name = 'flax-sentence-embeddings/all_datasets_v3_roberta-large'\n#     wikipediaModel = SentenceTransformer(model_name)\n#     wikipediaModel.to('cuda:0' if torch.cuda.is_available() else 'cpu')\n#     stop_words = set(stopwords.words('english'))\n#     lemma = WordNetLemmatizer()\n    \n# def load_model_third() :\n#     global embeddings , liar_classification\n#     embFinal = '../input/inputdata/emb_final.csv'\n#     df = pd.read_csv(embFinal)\n#     embeddings = df.values.tolist()\n#     liar_classification = pickle.load(open('../input/inputdata/liar_classification.sav', 'rb'))\n\ndef get_tweet(link) :\n    id = int(link.split('/')[-1])\n    tweetContent = twitterAPI.get_tweet(id)\n    tweetContent = str(tweetContent[0]).split()\n    return (\" \".join(tweetContent))\n\n# def prep(rowitem) :\n#     if len(str(rowitem).split()) < 10:\n#         return None\n#     rowitem = nltk.tokenize.word_tokenize(rowitem)\n#     rowitem = [i.lower() for i in rowitem if i.isalpha()]\n#     rowitem = [ i for i in rowitem if i not in stop_words ]\n#     rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n#     return rowitem\n\n# def sim(text, embeddings) :\n#     f = 0\n#     t = [prep(text)]\n#     if t[0] == None:\n#         return 'Too small'\n#     sen_embeddings = wikipediaModel.encode(t)\n# #     print('sen embedded')\n#     for idx, i in enumerate(embeddings) :\n#         sim = cosine_similarity(list(np.asarray(i).reshape(1, -1)),list(sen_embeddings))\n#         if sim > 0.8:\n#             f = 1\n#             return ('Present '+idx)\n#     if not f:\n#         return 'Not Present'\n\ndef detect_news(update, context) :\n    news = update.message.text\n\n    update.message.reply_text(\"Waiting for the output....\")\n\n    # if it's a link , treat it as tweet url\n\n    textToReply = prediction(news)\n    finalNewsFeatures = getNewsFeatures(news)\n    update.message.reply_text(textToReply)\n    update.message.reply_text(\"The news features are: \")\n    for key , val in finalNewsFeatures.items():\n        update.message.reply_text(key + \": \" + val)\n\ndef detect_image(update , context) :\n    photo_file = update.message.photo[-1].get_file()\n    photo_file.download('user_photo.jpg')\n    img_path = 'user_photo.jpg'\n    img_text = preprocess(get_text_from_image(img_path))\n    if len(img_text) > 0:\n        update.message.reply_text(\"Waiting for the output...\")\n        textToReply = prediction(img_text)\n        finalNewsFeatures = getNewsFeatures(news)\n        update.message.reply_text(textToReply)\n        update.message.reply_text(\"The news features are: \")\n        for key , val in finalNewsFeatures.items():\n            update.message.reply_text(key + \": \" + val)        \n    else:\n        update.message.reply_text(\"The model was not able to parse text from the given image\")\n\ndef preprocess(text) :\n    PRETRAINED_MODEL_PATH = './lid.176.bin'\n    model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n    return ' '.join([i  for i in text.split(' ') if len(i) != 1 if '__label__en' in model.predict(i, k=3)[0]])\n\ndef get_text_from_image(img_path) :\n    url = \"https://app.nanonets.com/api/v2/OCR/FullText\"\n    payload={'urls': ['MY_IMAGE_URL']}\n    files=[('file',('FILE_NAME',open(img_path,'rb'),'application/pdf'))]\n    headers = {}\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files, auth=requests.auth.HTTPBasicAuth('I-yhRSzNQmxj8dfhXKUQVA55Wj_1Sqjy', ''))\n\n    return json.loads(response.text)['results'][0]['page_data'][0]['raw_text']\n\ndef prediction(news) :\n    sentences=[news]\n    tokenized = cae_model_tokenizer(sentences, return_tensors=\"np\", padding=\"longest\")\n    outputs = classification_after_embedding_model(tokenized).logits\n    classifications = np.argmax(outputs, axis=1)\n    if classifications[0]==0 :\n        textToReply = \"The given news is FAKE\"\n    else :\n        textToReply = \"The given news is NOT FAKE\"\n    #finalNewsFeatures = getNewsFeatures(news)\n#     embedding_result = sim(news , embeddings)\n#     print(embedding_result)\n#     if embedding_result == 'Present':\n#         textToReply = \"This news is present\"\n# #         secondary_probab = classification_after_embedding.predict([news])\n# #         secondary_result = round(secondary_probab[0][0] , 2)\n# #         if secondary_result > 0.4:\n# #             textToReply = \"The given news is NOT FAKE\"\n# #         else:\n# #             textToReply = \"The given news is FAKE\"\n#     else:\n#         textToReply = \"This news is not present\"\n    # if result == 1:\n    #     textToReply = (\"The given news is NOT FAKE\")\n    # else:\n    #     print(\"Going to the second model\")\n    #     prediction = model_classify_second.predict([news])\n    #     prob = model_classify_second.predict_proba([news])\n    #     result = prediction[0]\n    #     if result == True:\n    #         textToReply = (\"The given news is NOT FAKE\")\n    #     else:\n    #         textToReply = (\"The given news is FAKE\")\n    return textToReply\n\n\ndef getNewsFeatures(inputText) :\n    finalNewsFeatures = {}\n    results_model1 = clickBaitModel(inputText)[0]\n    if(results_model1['label']=='Clickbait') :\n        finalNewsFeatures.__setitem__('Clickbait probability', str(round((results_model1['score']*100), 2))+\"%\")\n    else :\n        finalNewsFeatures.__setitem__('Clickbait probability', str(round(((1-results_model1['score'])*100), 2))+\"%\")\n    results_model2 = sentimentModel(inputText)[0]\n    finalNewsFeatures.__setitem__('Sentiment', results_model2['label'])\n    results_model3 = biasModel(inputText)[0]\n    if(results_model3['label']=='Biased') :\n        finalNewsFeatures.__setitem__('Biased percentage', str(round((results_model3['score']*100), 2))+\"%\")\n    else :\n        finalNewsFeatures.__setitem__('Biased percentage', str(round(((1-results_model3['score'])*100), 2))+\"%\")\n    return finalNewsFeatures","metadata":{"id":"8M_KXUw2ONWR","scrolled":true,"execution":{"iopub.status.busy":"2022-08-22T22:49:06.937051Z","iopub.execute_input":"2022-08-22T22:49:06.938219Z","iopub.status.idle":"2022-08-22T22:49:06.964304Z","shell.execute_reply.started":"2022-08-22T22:49:06.938177Z","shell.execute_reply":"2022-08-22T22:49:06.963218Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def main() :\n    load_models()\n    print(\"All models are loaded\")\n    TOKEN = \"5477065061:AAG9mOQ4W4Wus2nG1MmOSvOaO2Yp3fbAI2s\"\n    updater = Updater(token = TOKEN , use_context=True)\n    dp = updater.dispatcher\n    dp.add_handler(CommandHandler(\"start\", start))\n    dp.add_handler(CommandHandler(\"help\", help_command))\n    dp.add_handler(MessageHandler(Filters.text, detect_news))\n    dp.add_handler(MessageHandler(Filters.photo , detect_image))\n    updater.start_polling()\n    updater.idle()\n\n\nif __name__ == '__main__':\n    main()","metadata":{"id":"EW0tjQUDhnid","execution":{"iopub.status.busy":"2022-08-22T22:49:09.869790Z","iopub.execute_input":"2022-08-22T22:49:09.870454Z","iopub.status.idle":"2022-08-22T22:52:36.209842Z","shell.execute_reply.started":"2022-08-22T22:49:09.870418Z","shell.execute_reply":"2022-08-22T22:52:36.208588Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"GGiknpvI2e_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'flax-sentence-embeddings/all_datasets_v3_roberta-large'\nmodel = SentenceTransformer(model_name)\nmodel.to('cuda:0' if torch.cuda.is_available() else 'cpu')\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))\nlemma = WordNetLemmatizer()","metadata":{"id":"i7_Wj6S8qq91","execution":{"iopub.status.busy":"2022-08-22T19:40:20.475501Z","iopub.execute_input":"2022-08-22T19:40:20.476674Z","iopub.status.idle":"2022-08-22T19:41:35.135718Z","shell.execute_reply.started":"2022-08-22T19:40:20.476634Z","shell.execute_reply":"2022-08-22T19:41:35.134506Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"EMB_FILE_PTH = '../input/inputdata/emb_final.csv'\ndf = pd.read_csv(EMB_FILE_PTH)\nembeddings = df.values.tolist()","metadata":{"id":"4Mm3nXwZ0xwA","execution":{"iopub.status.busy":"2022-08-22T19:42:11.317504Z","iopub.execute_input":"2022-08-22T19:42:11.318451Z","iopub.status.idle":"2022-08-22T19:42:48.657438Z","shell.execute_reply.started":"2022-08-22T19:42:11.318409Z","shell.execute_reply":"2022-08-22T19:42:48.656376Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def prep(rowitem):\n    if len(str(rowitem).split()) < 10:\n        return None\n    rowitem = nltk.tokenize.word_tokenize(rowitem)\n    rowitem = [i.lower() for i in rowitem if i.isalpha()]\n    rowitem = [ i for i in rowitem if i not in stop_words ]\n    rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n\n    return rowitem","metadata":{"id":"sgjVmSus3bKL","execution":{"iopub.status.busy":"2022-08-22T19:42:48.659404Z","iopub.execute_input":"2022-08-22T19:42:48.660045Z","iopub.status.idle":"2022-08-22T19:42:48.667106Z","shell.execute_reply.started":"2022-08-22T19:42:48.660007Z","shell.execute_reply":"2022-08-22T19:42:48.665895Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def sim(text, embeddings):\n    f = 0\n    t = [prep(text)]\n    if t[0] == None:\n        return 'Too small'\n    sen_embeddings = model.encode(t)\n    for idx, i in enumerate(embeddings) :\n        sim = cosine_similarity(list(np.asarray(i).reshape(1, -1)),list(sen_embeddings))\n        if sim > 0.8:\n            f = 1\n            return ('Present at '+str(idx))\n\n    if not f:\n        return 'Not Present'","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:42:48.668758Z","iopub.execute_input":"2022-08-22T19:42:48.669167Z","iopub.status.idle":"2022-08-22T19:42:48.683887Z","shell.execute_reply.started":"2022-08-22T19:42:48.669131Z","shell.execute_reply":"2022-08-22T19:42:48.682807Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"text = input('Enter text ')\nprint(sim(text, embeddings))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:44:17.507517Z","iopub.execute_input":"2022-08-22T19:44:17.508677Z","iopub.status.idle":"2022-08-22T19:46:48.043384Z","shell.execute_reply.started":"2022-08-22T19:44:17.508630Z","shell.execute_reply":"2022-08-22T19:46:48.042226Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}