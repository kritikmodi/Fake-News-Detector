# -*- coding: utf-8 -*-
"""Telegram_bot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s2A53ebjM4k-CrcdsiPYC03-mY1jCbOG
"""

#!pip install python-telegram-bot --upgrade
#!pip install tensorflow-text
import logging
from telegram.ext import Updater , CommandHandler , MessageHandler , Filters
from fastai.vision.all import load_learner
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text

def start(update, context):
    update.message.reply_text(
        "EN : Just give me a news and I will tell you whether it is fake or not"
    )

def help_command(update, context):
    update.message.reply_text('My only purpose is to tell you whether a given news is fake or not')

def load_model():
    global model
    model = tf.keras.models.load_model('/content/drive/MyDrive/Telegram_bot/model.h5',custom_objects={'KerasLayer' : hub.KerasLayer})
    print("The model is loaded")


def detect_news(update, context):
    news = update.message.text
    result = model.predict([news])
    if result[0][0] > 0.7:
        update.message.reply_text("The given news is NOT FAKE and percentage is " + str(result[0][0]*100) + " %")
    if result[0][0] > 0.35 and result[0][0] < 0.7:
        update.message.reply_text("UNDETERMINED and the percentage is " + str(result[0][0] * 100) + "%")
    else:
        update.message.reply_text("The given news is FAKE and percentage is " + str((1-result[0][0])*100) + " %")
    
def main():
    load_model()
    TOKEN = "5477065061:AAFOxVFSTfnrwuSCbsxbUDtpwh3zXhTMk4Q"
    updater = Updater(token = TOKEN , use_context=True)
    dp = updater.dispatcher
    dp.add_handler(CommandHandler("start", start))
    dp.add_handler(CommandHandler("help", help_command))
    dp.add_handler(MessageHandler(Filters.text, detect_news))
    updater.start_polling()
    updater.idle()

if __name__ == '__main__':
    main()

!pip install transformers
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained('ritabrata/pibnews-distilbert')
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

import numpy as np
sentences = ['The previous government had bought about 1700 lakh metric tonnes of paddy during its five years.']
tokenized = tokenizer(sentences, return_tensors="np", padding="longest")

outputs = model(tokenized).logits

classifications = np.argmax(outputs, axis=1)
print(classifications)
