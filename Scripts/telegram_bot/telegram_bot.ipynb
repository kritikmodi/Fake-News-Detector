{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T10:36:20.085570Z","iopub.status.busy":"2022-08-23T10:36:20.085153Z","iopub.status.idle":"2022-08-23T10:38:48.136889Z","shell.execute_reply":"2022-08-23T10:38:48.136129Z","shell.execute_reply.started":"2022-08-23T10:36:20.085487Z"},"id":"d_1OdULI4jPH","scrolled":true,"trusted":true},"outputs":[],"source":["!pip install python-telegram-bot --upgrade\n","!pip install tensorflow-text\n","!pip install transformers\n","!pip install fasttext\n","!pip install scipy\n","!pip3 install \"scikit_learn==0.22.2.post1\"\n","!pip install tweepy==4.5.0\n","!pip install sentence-transformers\n","!pip install wikipedia"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:07:12.218484Z","iopub.status.busy":"2022-08-23T11:07:12.217667Z","iopub.status.idle":"2022-08-23T11:07:12.234970Z","shell.execute_reply":"2022-08-23T11:07:12.233693Z","shell.execute_reply.started":"2022-08-23T11:07:12.218444Z"},"id":"kQ-_gg6f26ix","trusted":true},"outputs":[],"source":["import requests\n","import base64\n","import json\n","import logging\n","import pandas as pd\n","from telegram.ext import Updater , CommandHandler , MessageHandler , Filters\n","from fastai.vision.all import load_learner\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import pipeline , AutoTokenizer, TFAutoModelForSequenceClassification\n","import fasttext\n","import numpy as np\n","import pickle\n","import spacy\n","from spacy import displacy\n","from collections import Counter\n","import en_core_web_sm\n","from pprint import pprint\n","import wikipedia as wiki\n","from nltk.util import ngrams\n","import gensim\n","import re\n","import nltk\n","import torch\n","from nltk import tokenize\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sentence_transformers import SentenceTransformer\n","import tweepy as tweepy\n","\n","# nltk.download('stopwords')\n","# nltk.download('omw-1.4')\n","nltk.download('punkt')\n","# nltk.download('wordnet')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T10:39:36.616895Z","iopub.status.busy":"2022-08-23T10:39:36.616145Z","iopub.status.idle":"2022-08-23T10:39:44.217730Z","shell.execute_reply":"2022-08-23T10:39:44.216616Z","shell.execute_reply.started":"2022-08-23T10:39:36.616859Z"},"id":"ultiAIXB50mk","trusted":true},"outputs":[],"source":["APIKey = \"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\"\n","!wget -O ./lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n","CONSUMER_KEY = \"eJL1xOgPnXVx0DzCr5pGa8lNv\"\n","CONSUMER_SECRET = \"iDBuPdCEXZQDzsRqvNtkVcIhcdvlT8x8aW74VTm1EqXcIaPmrZ\"\n","OAUTH_TOKEN = \"1420293020080082948-Qo8PBaf5oXA1xrPryabo3C3g09xdBf\"\n","OAUTH_TOKEN_SECRET = \"HzW0KllX3NRls7pOKA0cPIbNEyAFWON9wgVODcRwlrVBi\"\n","BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAABKRewEAAAAAem8kRGNx2U5tGTmD%2BtikqmENETI%3DCqzEWLXGEmM8WKNXRRnW0Tke4QlWw2sihgwtjpVYwnAR0QD6bo\"\n","twitterAPI = tweepy.Client(bearer_token = BEARER_TOKEN)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:41:50.833960Z","iopub.status.busy":"2022-08-23T11:41:50.833591Z","iopub.status.idle":"2022-08-23T11:41:50.870103Z","shell.execute_reply":"2022-08-23T11:41:50.869230Z","shell.execute_reply.started":"2022-08-23T11:41:50.833931Z"},"id":"8M_KXUw2ONWR","scrolled":true,"trusted":true},"outputs":[],"source":["def start(update, context) :\n","    update.message.reply_text(\n","        \"EN : Just give me a news and I will tell you whether it is FAKE or not\"\n","    )\n","\n","def help_command(update, context) :\n","    update.message.reply_text('My only purpose is to tell you whether a given news is fake or not')\n","\n","def load_models() :\n","    global clickBaitModel , sentimentModel , biasModel , classification_after_embedding_model, cae_model_tokenizer, liar_classification_model, wikipedia_model, nlp\n","    classification_after_embedding_model = TFAutoModelForSequenceClassification.from_pretrained('pururaj/Test_model')\n","    cae_model_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n","    clickBaitModel = pipeline(model=\"elozano/bert-base-cased-clickbait-news\", tokenizer=\"elozano/bert-base-cased-clickbait-news\")\n","    sentimentModel = pipeline(model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n","    biasModel = pipeline(model=\"d4data/bias-detection-model\", tokenizer=\"d4data/bias-detection-model\")\n","    liar_classification_model = pickle.load(open('../input/inputdata/liar_classification.sav', 'rb'))\n","    wikipedia_model = SentenceTransformer('bert-base-nli-mean-tokens')\n","    nlp = en_core_web_sm.load()\n","\n","# def load_model_second() :\n","#     global wikipediaModel , stop_words , lemma\n","#     model_name = 'flax-sentence-embeddings/all_datasets_v3_roberta-large'\n","#     wikipediaModel = SentenceTransformer(model_name)\n","#     wikipediaModel.to('cuda:0' if torch.cuda.is_available() else 'cpu')\n","#     stop_words = set(stopwords.words('english'))\n","#     lemma = WordNetLemmatizer()\n","    \n","# def load_model_third() :\n","#     global embeddings , liar_classification\n","#     embFinal = '../input/inputdata/emb_final.csv'\n","#     df = pd.read_csv(embFinal)\n","#     embeddings = df.values.tolist()\n","#     liar_classification = pickle.load(open('../input/inputdata/liar_classification.sav', 'rb'))\n","\n","def keywords(sentence):\n","\n","    doc = nlp(sentence)\n","\n","    results = ([(X.text, X.label_)[0] for X in doc.ents])\n","    return list(set(results))\n","\n","def clean_wiki_text(text):\n","    text = re.sub(r'==.+==', '.', text)\n","    text = re.sub(r'\\n', ' ', text)\n","    text = re.sub(r'\\t', ' ', text)\n","    text = re.sub(r'\\[[0-9]+\\]', ' ', text)\n","    text = re.sub(r' +', ' ', text)\n","    text = re.sub(r'\\. \\.', '.', text)\n","    return text\n","\n","def content(claim, results):\n","    sentences = []\n","    found=[]\n","    for i in results:\n","        try :\n","            current_page = wiki.page(i)\n","            if current_page not in found:\n","                found.append(current_page)\n","        except :\n","            continue\n","    titles=[i.title for i in found]\n","    titles=[i[0] for i in topNSimilar(claim, titles)]\n","    for i in found:\n","        if i.title not in titles:\n","            found.remove(i)\n","\n","    for i in found:\n","        current_content = i.content\n","        sentences.extend(tokenize.sent_tokenize(clean_wiki_text(current_content)))\n","    return sentences\n","\n","def topNSimilar(claim, sentList, n=5):\n","    distList=[]\n","    for i in sentList:\n","        document_term_matrix = TfidfVectorizer().fit_transform([i, claim])\n","        dist = document_term_matrix * document_term_matrix.transpose()\n","        distance = dist.toarray()[0][1]\n","\n","        if len(distList)<=n:\n","            distList.append([i, distance])\n","        else:\n","            distList=sorted(distList, key=lambda x: x[1], reverse=True)\n","            if distance>distList[-1][1]:\n","                distList.pop()\n","                distList.append([i, distance])\n","    return sorted(distList, key=lambda x: x[1], reverse=True)\n","\n","def topNBert(claim, res):\n","    topNFacts=[i[0] for i in res]\n","    topNScore=[cosine_similarity( [wikipedia_model.encode(claim)], [wikipedia_model.encode(i)] )[0][0] for i in topNFacts]\n","    topN=zip(topNFacts, topNScore)\n","    return [list(i) for i in list(topN)]\n","\n","def get_tweet(link) :\n","    id = int(link.split('/')[-1])\n","    tweetContent = twitterAPI.get_tweet(id)\n","    tweetContent = str(tweetContent[0]).split()\n","    return (\" \".join(tweetContent))\n","\n","# def prep(rowitem) :\n","#     if len(str(rowitem).split()) < 10:\n","#         return None\n","#     rowitem = nltk.tokenize.word_tokenize(rowitem)\n","#     rowitem = [i.lower() for i in rowitem if i.isalpha()]\n","#     rowitem = [ i for i in rowitem if i not in stop_words ]\n","#     rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n","#     return rowitem\n","\n","# def sim(text, embeddings) :\n","#     f = 0\n","#     t = [prep(text)]\n","#     if t[0] == None:\n","#         return 'Too small'\n","#     sen_embeddings = wikipediaModel.encode(t)\n","# #     print('sen embedded')\n","#     for idx, i in enumerate(embeddings) :\n","#         sim = cosine_similarity(list(np.asarray(i).reshape(1, -1)),list(sen_embeddings))\n","#         if sim > 0.8:\n","#             f = 1\n","#             return ('Present '+idx)\n","#     if not f:\n","#         return 'Not Present'\n","\n","def detect_news(update, context) :\n","    news = update.message.text\n","\n","    update.message.reply_text(\"Waiting for the output....\")\n","\n","    # if it's a link , treat it as tweet url\n","\n","    textToReply = prediction(news)\n","    finalNewsFeatures = getNewsFeatures(news)\n","    finalWikipediaResults = topNBert(news, topNSimilar(news, content(news, keywords(news))))\n","    kws=keywords(news)\n","    cnt=content(news,kws)\n","    tns=topNSimilar(news,cnt)\n","    tnb=topNBert(news,tns)\n","    \n","    update.message.reply_text(textToReply)\n","    update.message.reply_text(\"The news features are: \")\n","    for key , val in finalNewsFeatures.items():\n","        update.message.reply_text(key + \": \" + val)\n","    update.message.reply_text(\"We found these related articles on the web :\")\n","    for i in tnb :\n","        update.message.reply_text(i[0])\n","\n","def detect_image(update , context) :\n","    photo_file = update.message.photo[-1].get_file()\n","    photo_file.download('user_photo.jpg')\n","    img_path = 'user_photo.jpg'\n","    img_text = preprocess(get_text_from_image(img_path))\n","    if len(img_text) > 0:\n","        update.message.reply_text(\"Waiting for the output...\")\n","        textToReply = prediction(img_text)\n","        finalNewsFeatures = getNewsFeatures(news)\n","        update.message.reply_text(textToReply)\n","        update.message.reply_text(\"The news features are: \")\n","        for key , val in finalNewsFeatures.items():\n","            update.message.reply_text(key + \": \" + val)        \n","    else:\n","        update.message.reply_text(\"The model was not able to parse text from the given image\")\n","\n","def preprocess(text) :\n","    PRETRAINED_MODEL_PATH = './lid.176.bin'\n","    model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n","    return ' '.join([i  for i in text.split(' ') if len(i) != 1 if '__label__en' in model.predict(i, k=3)[0]])\n","\n","def get_text_from_image(img_path) :\n","    url = \"https://app.nanonets.com/api/v2/OCR/FullText\"\n","    payload={'urls': ['MY_IMAGE_URL']}\n","    files=[('file',('FILE_NAME',open(img_path,'rb'),'application/pdf'))]\n","    headers = {}\n","\n","    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files, auth=requests.auth.HTTPBasicAuth('I-yhRSzNQmxj8dfhXKUQVA55Wj_1Sqjy', ''))\n","\n","    return json.loads(response.text)['results'][0]['page_data'][0]['raw_text']\n","\n","def prediction(news) :\n","    sentences=[news]\n","    tokenized = cae_model_tokenizer(sentences, return_tensors=\"np\", padding=\"longest\")\n","    outputs = classification_after_embedding_model(tokenized).logits\n","    classifications = np.argmax(outputs, axis=1)\n","    if classifications[0]==0 :\n","        textToReply = \"The given news is FAKE\"\n","    else :\n","        textToReply = \"The given news is NOT FAKE\"\n","    return textToReply\n","\n","def getNewsFeatures(inputText) :\n","    finalNewsFeatures = {}\n","    results_model1 = clickBaitModel(inputText)[0]\n","    if(results_model1['label']=='Clickbait') :\n","        finalNewsFeatures.__setitem__('Clickbait probability', str(round((results_model1['score']*100), 2))+\"%\")\n","    else :\n","        finalNewsFeatures.__setitem__('Clickbait probability', str(round(((1-results_model1['score'])*100), 2))+\"%\")\n","    results_model2 = sentimentModel(inputText)[0]\n","    finalNewsFeatures.__setitem__('Sentiment', results_model2['label'])\n","    results_model3 = biasModel(inputText)[0]\n","    if(results_model3['label']=='Biased') :\n","        finalNewsFeatures.__setitem__('Biased percentage', str(round((results_model3['score']*100), 2))+\"%\")\n","    else :\n","        finalNewsFeatures.__setitem__('Biased percentage', str(round(((1-results_model3['score'])*100), 2))+\"%\")\n","    return finalNewsFeatures"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T11:41:51.080589Z","iopub.status.busy":"2022-08-23T11:41:51.079673Z","iopub.status.idle":"2022-08-23T11:44:55.945346Z","shell.execute_reply":"2022-08-23T11:44:55.944054Z","shell.execute_reply.started":"2022-08-23T11:41:51.080545Z"},"id":"EW0tjQUDhnid","scrolled":true,"trusted":true},"outputs":[],"source":["def main() :\n","    #load_models()\n","    print(\"All models are loaded\")\n","    TOKEN = \"5477065061:AAG9mOQ4W4Wus2nG1MmOSvOaO2Yp3fbAI2s\"\n","    updater = Updater(token = TOKEN , use_context=True)\n","    dp = updater.dispatcher\n","    dp.add_handler(CommandHandler(\"start\", start))\n","    dp.add_handler(CommandHandler(\"help\", help_command))\n","    dp.add_handler(MessageHandler(Filters.text, detect_news))\n","    dp.add_handler(MessageHandler(Filters.photo , detect_image))\n","    updater.start_polling()\n","    updater.idle()\n","\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GGiknpvI2e_S"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-08-22T19:40:20.476674Z","iopub.status.busy":"2022-08-22T19:40:20.475501Z","iopub.status.idle":"2022-08-22T19:41:35.135718Z","shell.execute_reply":"2022-08-22T19:41:35.134506Z","shell.execute_reply.started":"2022-08-22T19:40:20.476634Z"},"id":"i7_Wj6S8qq91","trusted":true},"outputs":[],"source":["model_name = 'flax-sentence-embeddings/all_datasets_v3_roberta-large'\n","model = SentenceTransformer(model_name)\n","model.to('cuda:0' if torch.cuda.is_available() else 'cpu')\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","stop_words = set(stopwords.words('english'))\n","lemma = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-08-22T19:42:11.318451Z","iopub.status.busy":"2022-08-22T19:42:11.317504Z","iopub.status.idle":"2022-08-22T19:42:48.657438Z","shell.execute_reply":"2022-08-22T19:42:48.656376Z","shell.execute_reply.started":"2022-08-22T19:42:11.318409Z"},"id":"4Mm3nXwZ0xwA","trusted":true},"outputs":[],"source":["EMB_FILE_PTH = '../input/inputdata/emb_final.csv'\n","df = pd.read_csv(EMB_FILE_PTH)\n","embeddings = df.values.tolist()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-22T19:42:48.660045Z","iopub.status.busy":"2022-08-22T19:42:48.659404Z","iopub.status.idle":"2022-08-22T19:42:48.667106Z","shell.execute_reply":"2022-08-22T19:42:48.665895Z","shell.execute_reply.started":"2022-08-22T19:42:48.660007Z"},"id":"sgjVmSus3bKL","trusted":true},"outputs":[],"source":["def prep(rowitem):\n","    if len(str(rowitem).split()) < 10:\n","        return None\n","    rowitem = nltk.tokenize.word_tokenize(rowitem)\n","    rowitem = [i.lower() for i in rowitem if i.isalpha()]\n","    rowitem = [ i for i in rowitem if i not in stop_words ]\n","    rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n","\n","    return rowitem"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-08-22T19:42:48.669167Z","iopub.status.busy":"2022-08-22T19:42:48.668758Z","iopub.status.idle":"2022-08-22T19:42:48.683887Z","shell.execute_reply":"2022-08-22T19:42:48.682807Z","shell.execute_reply.started":"2022-08-22T19:42:48.669131Z"},"trusted":true},"outputs":[],"source":["def sim(text, embeddings):\n","    f = 0\n","    t = [prep(text)]\n","    if t[0] == None:\n","        return 'Too small'\n","    sen_embeddings = model.encode(t)\n","    for idx, i in enumerate(embeddings) :\n","        sim = cosine_similarity(list(np.asarray(i).reshape(1, -1)),list(sen_embeddings))\n","        if sim > 0.8:\n","            f = 1\n","            return ('Present at '+str(idx))\n","\n","    if not f:\n","        return 'Not Present'"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-08-22T19:44:17.508677Z","iopub.status.busy":"2022-08-22T19:44:17.507517Z","iopub.status.idle":"2022-08-22T19:46:48.043384Z","shell.execute_reply":"2022-08-22T19:46:48.042226Z","shell.execute_reply.started":"2022-08-22T19:44:17.508630Z"},"trusted":true},"outputs":[],"source":["text = input('Enter text ')\n","print(sim(text, embeddings))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
