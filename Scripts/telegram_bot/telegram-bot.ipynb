{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install python-telegram-bot --upgrade\n!pip install tensorflow-text\n!pip install transformers\n!pip install fasttext\n!pip install scipy\n!pip3 install \"scikit_learn==0.22.2.post1\"\n!pip install tweepy==4.5.0\n!pip install sentence-transformers\n!pip install wikipedia\n!pip install sentencepiece","metadata":{"id":"d_1OdULI4jPH","scrolled":true,"execution":{"iopub.status.busy":"2022-08-23T22:59:29.801306Z","iopub.execute_input":"2022-08-23T22:59:29.801834Z","iopub.status.idle":"2022-08-23T23:03:18.509505Z","shell.execute_reply.started":"2022-08-23T22:59:29.801733Z","shell.execute_reply":"2022-08-23T23:03:18.507005Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import requests\nimport base64\nimport json\nimport logging\nimport pandas as pd\nfrom telegram.ext import Updater , CommandHandler , MessageHandler , Filters\nfrom fastai.vision.all import load_learner\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom transformers import pipeline , AutoTokenizer, TFAutoModelForSequenceClassification\nimport fasttext\nimport numpy as np\nimport pickle\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nfrom pprint import pprint\nimport wikipedia as wiki\nfrom nltk.util import ngrams\nimport gensim\nimport re\nimport nltk\nimport torch\nfrom nltk import tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sentence_transformers import SentenceTransformer\nimport tweepy as tweepy\n\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"id":"kQ-_gg6f26ix","execution":{"iopub.status.busy":"2022-08-23T23:08:05.443118Z","iopub.execute_input":"2022-08-23T23:08:05.443935Z","iopub.status.idle":"2022-08-23T23:08:18.667842Z","shell.execute_reply.started":"2022-08-23T23:08:05.443867Z","shell.execute_reply":"2022-08-23T23:08:18.665987Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"APIKey = \"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\"\n!wget -O ./lid.176.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\nCONSUMER_KEY = \"eJL1xOgPnXVx0DzCr5pGa8lNv\"\nCONSUMER_SECRET = \"iDBuPdCEXZQDzsRqvNtkVcIhcdvlT8x8aW74VTm1EqXcIaPmrZ\"\nOAUTH_TOKEN = \"1420293020080082948-Qo8PBaf5oXA1xrPryabo3C3g09xdBf\"\nOAUTH_TOKEN_SECRET = \"HzW0KllX3NRls7pOKA0cPIbNEyAFWON9wgVODcRwlrVBi\"\nBEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAABKRewEAAAAAem8kRGNx2U5tGTmD%2BtikqmENETI%3DCqzEWLXGEmM8WKNXRRnW0Tke4QlWw2sihgwtjpVYwnAR0QD6bo\"\ntwitterAPI = tweepy.Client(bearer_token = BEARER_TOKEN)","metadata":{"id":"ultiAIXB50mk","execution":{"iopub.status.busy":"2022-08-23T23:08:18.671516Z","iopub.execute_input":"2022-08-23T23:08:18.672552Z","iopub.status.idle":"2022-08-23T23:08:21.558762Z","shell.execute_reply.started":"2022-08-23T23:08:18.672501Z","shell.execute_reply":"2022-08-23T23:08:21.556982Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def start(update, context) :\n    update.message.reply_text(\n        \"EN : Just give me a news and I will tell you whether it is FAKE or not\"\n    )\n\ndef help_command(update, context) :\n    update.message.reply_text('My only purpose is to tell you whether a given news is fake or not')\n\ndef load_models() :\n    global clickBaitModel , sentimentModel , biasModel , classification_after_embedding_model, cae_model_tokenizer, liar_classification_model, wikipedia_model, nlp, d2v_model\n    classification_after_embedding_model = TFAutoModelForSequenceClassification.from_pretrained('pururaj/Test_model')\n    cae_model_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n    clickBaitModel = pipeline(model=\"elozano/bert-base-cased-clickbait-news\", tokenizer=\"elozano/bert-base-cased-clickbait-news\")\n    sentimentModel = pipeline(model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\")\n    biasModel = pipeline(model=\"d4data/bias-detection-model\", tokenizer=\"d4data/bias-detection-model\")\n    liar_classification_model = pickle.load(open('../input/inputdata/liar_classification.sav', 'rb'))\n    wikipedia_model = SentenceTransformer('bert-base-nli-mean-tokens')\n    nlp = en_core_web_sm.load()\n    d2v_model = gensim.models.doc2vec.Doc2Vec.load('../input/d2v-model/d2v.model')\n\ndef keywords(sentence) :\n\n    doc = nlp(sentence)\n\n    results = ([(X.text, X.label_)[0] for X in doc.ents])\n    return list(set(results))\n\ndef clean_wiki_text(text) :\n    text = re.sub(r'==.+==', '.', text)\n    text = re.sub(r'\\n', ' ', text)\n    text = re.sub(r'\\t', ' ', text)\n    text = re.sub(r'\\[[0-9]+\\]', ' ', text)\n    text = re.sub(r' +', ' ', text)\n    text = re.sub(r'\\. \\.', '.', text)\n    return text\n\ndef content(claim, results) :\n    sentences = []\n    found=[]\n    for i in results:\n        try :\n            current_page = wiki.page(i)\n            if current_page not in found:\n                found.append(current_page)\n        except :\n            continue\n    titles=[i.title for i in found]\n    titles=[i[0] for i in topNSimilar(claim, titles)]\n    for i in found:\n        if i.title not in titles:\n            found.remove(i)\n\n    for i in found:\n        current_content = i.content\n        sentences.extend(tokenize.sent_tokenize(clean_wiki_text(current_content)))\n    return sentences\n\ndef topNSimilar(claim, sentList, n=5) :\n    distList=[]\n    for i in sentList:\n        document_term_matrix = TfidfVectorizer().fit_transform([i, claim])\n        dist = document_term_matrix * document_term_matrix.transpose()\n        distance = dist.toarray()[0][1]\n\n        if len(distList)<=n:\n            distList.append([i, distance])\n        else:\n            distList=sorted(distList, key=lambda x: x[1], reverse=True)\n            if distance>distList[-1][1]:\n                distList.pop()\n                distList.append([i, distance])\n    return sorted(distList, key=lambda x: x[1], reverse=True)\n\ndef topNBert(claim, res) :\n    topNFacts=[i[0] for i in res]\n    topNScore=[cosine_similarity( [wikipedia_model.encode(claim)], [wikipedia_model.encode(i)] )[0][0] for i in topNFacts]\n    topN=zip(topNFacts, topNScore)\n    return [list(i) for i in list(topN)]\n\ndef get_tweet(link) :\n    twt_id = (link.split('/')[-1])\n    twt_id = re.sub(r'\\?.+', '', twt_id)\n    tweetContent = twitterAPI.get_tweet(twt_id)\n    tweetContent = str(tweetContent[0]).split()\n    return (\" \".join(tweetContent))\n\ndef prep(rowitem) :\n    if len(str(rowitem).split()) < 10:\n        return None\n    rowitem = nltk.tokenize.word_tokenize(rowitem)\n    rowitem = [i.lower() for i in rowitem if i.isalpha()]\n    rowitem = [ i for i in rowitem if i not in stop_words ]\n    rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n\n    return rowitem\n\ndef detect_news(update, context) :\n    news = update.message.text\n\n    update.message.reply_text(\"Waiting for the output....\")\n\n    if news[:5]=='https' :\n        news = get_tweet(news)\n    textToReply = prediction(news)\n    finalNewsFeatures = getNewsFeatures(news)\n    finalWikipediaResults = topNBert(news, topNSimilar(news, content(news, keywords(news))))\n    kws=keywords(news)\n    cnt=content(news,kws)\n    tns=topNSimilar(news,cnt)\n    tnb=topNBert(news,tns)\n    \n    update.message.reply_text(textToReply)\n    update.message.reply_text(\"The news features are: \")\n    for key , val in finalNewsFeatures.items():\n        update.message.reply_text(key + \": \" + val)\n    update.message.reply_text(\"We found these related articles on the web :\")\n    for i in tnb :\n        update.message.reply_text(i[0])\n\ndef detect_image(update , context) :\n    photo_file = update.message.photo[-1].get_file()\n    photo_file.download('user_photo.jpg')\n    img_path = 'user_photo.jpg'\n    news = preprocess(get_text_from_image(img_path))\n    if len(news) > 0 : \n        update.message.reply_text(\"Waiting for the output....\")\n\n        textToReply = prediction(news)\n        finalNewsFeatures = getNewsFeatures(news)\n        finalWikipediaResults = topNBert(news, topNSimilar(news, content(news, keywords(news))))\n        kws=keywords(news)\n        cnt=content(news,kws)\n        tns=topNSimilar(news,cnt)\n        tnb=topNBert(news,tns)\n    \n        update.message.reply_text(textToReply)\n        update.message.reply_text(\"The news features are: \")\n        for key , val in finalNewsFeatures.items():\n            update.message.reply_text(key + \": \" + val)\n        update.message.reply_text(\"We found these related articles on the web :\")\n        for i in tnb :\n            update.message.reply_text(i[0])\n                \n    else:\n        update.message.reply_text(\"The model was not able to parse text from the given image\")\n\ndef preprocess(text) :\n    PRETRAINED_MODEL_PATH = './lid.176.bin'\n    model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n    return ' '.join([i  for i in text.split(' ') if len(i) != 1 if '__label__en' in model.predict(i, k=3)[0]])\n\ndef get_text_from_image(img_path) :\n    url = \"https://app.nanonets.com/api/v2/OCR/FullText\"\n    payload={'urls': ['MY_IMAGE_URL']}\n    files=[('file',('FILE_NAME',open(img_path,'rb'),'application/pdf'))]\n    headers = {}\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files, auth=requests.auth.HTTPBasicAuth('I-yhRSzNQmxj8dfhXKUQVA55Wj_1Sqjy', ''))\n\n    return json.loads(response.text)['results'][0]['page_data'][0]['raw_text']\n\ndef prediction(news) :\n    test_data = word_tokenize(news.lower())\n    v1 = d2v_model.infer_vector(test_data)\n    similar_doc = d2v_model.docvecs.most_similar(positive=[v1])\n    print(similar_doc)\n    if similar_doc[0][1]>=0.9 :\n        sentences=[news]\n        tokenized = cae_model_tokenizer(sentences, return_tensors=\"np\", padding=\"longest\")\n        outputs = classification_after_embedding_model(tokenized).logits\n        classifications = np.argmax(outputs, axis=1)\n        if classifications[0]==0 :\n            textToReply = \"The given news is FAKE\"\n        else :\n            textToReply = \"The given news is NOT FAKE\"\n    else :\n        textToReply = \"Will go the LR classification model\"\n    return textToReply\n\ndef getNewsFeatures(inputText) :\n    finalNewsFeatures = {}\n    results_model1 = clickBaitModel(inputText)[0]\n    if(results_model1['label']=='Clickbait') :\n        finalNewsFeatures.__setitem__('Clickbait probability', str(round((results_model1['score']*100), 2))+\"%\")\n    else :\n        finalNewsFeatures.__setitem__('Clickbait probability', str(round(((1-results_model1['score'])*100), 2))+\"%\")\n    results_model2 = sentimentModel(inputText)[0]\n    finalNewsFeatures.__setitem__('Sentiment', results_model2['label'])\n    results_model3 = biasModel(inputText)[0]\n    if(results_model3['label']=='Biased') :\n        finalNewsFeatures.__setitem__('Biased percentage', str(round((results_model3['score']*100), 2))+\"%\")\n    else :\n        finalNewsFeatures.__setitem__('Biased percentage', str(round(((1-results_model3['score'])*100), 2))+\"%\")\n    return finalNewsFeatures","metadata":{"id":"8M_KXUw2ONWR","scrolled":true,"execution":{"iopub.status.busy":"2022-08-23T23:20:46.635508Z","iopub.execute_input":"2022-08-23T23:20:46.636365Z","iopub.status.idle":"2022-08-23T23:20:46.687544Z","shell.execute_reply.started":"2022-08-23T23:20:46.636307Z","shell.execute_reply":"2022-08-23T23:20:46.686017Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def main() :\n    #load_models()\n    print(\"All models are loaded\")\n    TOKEN = \"5477065061:AAG9mOQ4W4Wus2nG1MmOSvOaO2Yp3fbAI2s\"\n    updater = Updater(token = TOKEN , use_context=True)\n    dp = updater.dispatcher\n    dp.add_handler(CommandHandler(\"start\", start))\n    dp.add_handler(CommandHandler(\"help\", help_command))\n    dp.add_handler(MessageHandler(Filters.text, detect_news))\n    dp.add_handler(MessageHandler(Filters.photo , detect_image))\n    updater.start_polling()\n    updater.idle()\n\nif __name__ == '__main__':\n    main()","metadata":{"id":"EW0tjQUDhnid","scrolled":true,"execution":{"iopub.status.busy":"2022-08-23T23:20:46.832997Z","iopub.execute_input":"2022-08-23T23:20:46.833946Z","iopub.status.idle":"2022-08-23T23:27:01.523825Z","shell.execute_reply.started":"2022-08-23T23:20:46.833896Z","shell.execute_reply":"2022-08-23T23:27:01.522445Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'flax-sentence-embeddings/all_datasets_v3_roberta-large'\nmodel = SentenceTransformer(model_name)\nmodel.to('cuda:0' if torch.cuda.is_available() else 'cpu')\nnltk.download('stopwords')\nnltk.download('omw-1.4')\nnltk.download('punkt')\nnltk.download('wordnet')\nstop_words = set(stopwords.words('english'))\nlemma = WordNetLemmatizer()","metadata":{"id":"i7_Wj6S8qq91","execution":{"iopub.status.busy":"2022-08-22T19:40:20.475501Z","iopub.execute_input":"2022-08-22T19:40:20.476674Z","iopub.status.idle":"2022-08-22T19:41:35.135718Z","shell.execute_reply.started":"2022-08-22T19:40:20.476634Z","shell.execute_reply":"2022-08-22T19:41:35.134506Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"EMB_FILE_PTH = '../input/inputdata/emb_final.csv'\ndf = pd.read_csv(EMB_FILE_PTH)\nembeddings = df.values.tolist()","metadata":{"id":"4Mm3nXwZ0xwA","execution":{"iopub.status.busy":"2022-08-22T19:42:11.317504Z","iopub.execute_input":"2022-08-22T19:42:11.318451Z","iopub.status.idle":"2022-08-22T19:42:48.657438Z","shell.execute_reply.started":"2022-08-22T19:42:11.318409Z","shell.execute_reply":"2022-08-22T19:42:48.656376Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def prep(rowitem):\n    if len(str(rowitem).split()) < 10:\n        return None\n    rowitem = nltk.tokenize.word_tokenize(rowitem)\n    rowitem = [i.lower() for i in rowitem if i.isalpha()]\n    rowitem = [ i for i in rowitem if i not in stop_words ]\n    rowitem = ' '.join([ lemma.lemmatize(i) for i in rowitem ])\n\n    return rowitem","metadata":{"id":"sgjVmSus3bKL","execution":{"iopub.status.busy":"2022-08-22T19:42:48.659404Z","iopub.execute_input":"2022-08-22T19:42:48.660045Z","iopub.status.idle":"2022-08-22T19:42:48.667106Z","shell.execute_reply.started":"2022-08-22T19:42:48.660007Z","shell.execute_reply":"2022-08-22T19:42:48.665895Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def sim(text, embeddings):\n    f = 0\n    t = [prep(text)]\n    if t[0] == None:\n        return 'Too small'\n    sen_embeddings = model.encode(t)\n    for idx, i in enumerate(embeddings) :\n        sim = cosine_similarity(list(np.asarray(i).reshape(1, -1)),list(sen_embeddings))\n        if sim > 0.8:\n            f = 1\n            return ('Present at '+str(idx))\n\n    if not f:\n        return 'Not Present'","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:42:48.668758Z","iopub.execute_input":"2022-08-22T19:42:48.669167Z","iopub.status.idle":"2022-08-22T19:42:48.683887Z","shell.execute_reply.started":"2022-08-22T19:42:48.669131Z","shell.execute_reply":"2022-08-22T19:42:48.682807Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"text = input('Enter text ')\nprint(sim(text, embeddings))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:44:17.507517Z","iopub.execute_input":"2022-08-22T19:44:17.508677Z","iopub.status.idle":"2022-08-22T19:46:48.043384Z","shell.execute_reply.started":"2022-08-22T19:44:17.508630Z","shell.execute_reply":"2022-08-22T19:46:48.042226Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}